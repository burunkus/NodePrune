{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvv3ySZOPRu4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Multiply, Flatten, Reshape, Conv2D, MaxPooling2D, Add\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rc, rcParams\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3H3T1hFS0A9"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "#tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_PjRhpzVBbv"
   },
   "outputs": [],
   "source": [
    "# Check tensorflow version\n",
    "#tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiyzW0ZWVDPN"
   },
   "outputs": [],
   "source": [
    "# check state of GPUs\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoW-2LmTTc4i"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox_qzlQpS1wy"
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    A helper class for debugging.\n",
    "    \"\"\"\n",
    "    def __init__(self, output=False):\n",
    "        self.output = output\n",
    "    \n",
    "    def log(self, message, value):\n",
    "        if self.output:\n",
    "            print(message, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-1yT9lXVJBN"
   },
   "source": [
    "## Section 0: NodePrune class contains methods for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFU-MctFVTwV"
   },
   "outputs": [],
   "source": [
    "class NodePrune(object):\n",
    "    def flatten_mask(self, mask):\n",
    "        \"\"\"Returns the nodes of mask flattened\n",
    "        Input:\n",
    "        mask - the mask position, in dictionary form\n",
    "        Output:\n",
    "        mask_flatten - the mask position, flattened out in 1D array\n",
    "        \"\"\"\n",
    "        mask_flatten = []\n",
    "        for k, v in mask.items():\n",
    "            current_mask = v.flatten()\n",
    "            mask_flatten = np.hstack([mask_flatten, current_mask]) #append all mask. i.e make a total of 1D array of all mask\n",
    "\n",
    "        return mask_flatten\n",
    "\n",
    "    def reset_mask(self, mask):\n",
    "        \"\"\"Resets mask to initial start state of all ones\"\"\"\n",
    "        for k, v in mask.items():\n",
    "            mask[k] = np.ones_like(v)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def compare_mask(self, mask1, mask2):\n",
    "        \"\"\" Compares how similar both masks (mask1, mask2) are and returns a percentage similarity \"\"\"\n",
    "        count = 0\n",
    "        total_count = 0\n",
    "        for k, v in mask1.items():\n",
    "            count += np.sum(mask1[k] == mask2[k])\n",
    "            total_count += len(mask1[k].ravel())\n",
    "            \n",
    "        return count/total_count\n",
    "\n",
    "    def percent_mask(self, mask):\n",
    "        \"\"\"Returns the percentage of mask that contains 1s\"\"\"\n",
    "        num_mask = 0\n",
    "        total_mask = 0\n",
    "        \n",
    "        for v in mask.values():\n",
    "            num_mask += np.sum(v)\n",
    "            total_mask += len(v.ravel())\n",
    "            \n",
    "        return num_mask/total_mask\n",
    "\n",
    "    def save_file(self, cache, name):\n",
    "        \"\"\" Function which saves the cache \"\"\"\n",
    "        with open('Caches/'+name+'.p','wb') as outfile:\n",
    "            pickle.dump(cache, outfile)\n",
    "        \n",
    "    def load_file(self, name):\n",
    "        \"\"\" Function which loads the cache \"\"\"\n",
    "        with open('Caches/'+name+'.p','rb') as infile: \n",
    "            cache = pickle.load(infile)\n",
    "        return cache\n",
    "\n",
    "    def hoyer_square(self, weight):\n",
    "        \"\"\"\n",
    "        Computes the square of hoyer regularization\n",
    "        Args:\n",
    "            weights: the weights in a given layer\n",
    "        Returns: \n",
    "            hoyer_square: A scalar that penalizes the weights \n",
    "        \"\"\"\n",
    "        numerator = np.sum(np.abs(weight)) ** 2\n",
    "        denominator = np.sum(weight ** 2)\n",
    "        return numerator / denominator \n",
    "\n",
    "    def add_mask(self, mask, layer, layer_index, activation_array, initialize=True, in_out_layers=False):\n",
    "        \"\"\"Initialize mask for layers in MLPs.\n",
    "        Args:\n",
    "            layer: tf.keras layer\n",
    "            mask: Dict - key is a layer, value is numpy array of containing \n",
    "                0s(nodes to be removed) or 1s(nodes to keep)\n",
    "            activation_array: array of activations\n",
    "            layer_index: Int - used track the position of each layer\n",
    "            initialize: Boolean - If true initializes mask to all 1s, else uses the\n",
    "                        mask corresponding to the current layer(i.e model) to drop \n",
    "                        nodes in the layer by doing element wise multiplication. \n",
    "            in_out_layers: Boolean - Indicates if mask should be created for input and \n",
    "                        output layers. Defaults to False as we are not dropping the\n",
    "                        input and output nodes\n",
    "        Returns:\n",
    "            mask: Dict - key is a layer, value is numpy array of mask\n",
    "            layer: updated tf.keras layer with mask\n",
    "            activation_array: list of the post activations of each node in a layer\n",
    "            layer_index: Int - unique number for naming layers \n",
    "        \"\"\"\n",
    "\n",
    "        layer_name = 'layer_' + str(layer_index)\n",
    "\n",
    "        if initialize:\n",
    "            if in_out_layers == False: # only initialize mask when it is not the input or output layer\n",
    "                mask[layer_name] = np.ones(layer.shape[1:])\n",
    "        \n",
    "        if in_out_layers == True:\n",
    "            layer = Multiply()([layer, tf.multiply(tf.ones_like(layer), tf.ones_like(layer))]) # Does nothing but retain the layer the way it was\n",
    "        else:\n",
    "            # apply mask to the layer to drop or retain nodes\n",
    "            layer = Multiply()([layer, tf.multiply(tf.ones_like(layer), mask[layer_name])]) # tf.multiply converts to inputs same type - Tensors and ensures they are of same size by usig broadcasting\n",
    "        \n",
    "        activation_array.append(layer)\n",
    "        layer_index += 1\n",
    "\n",
    "        return mask, layer, activation_array, layer_index\n",
    "\n",
    "    def add_filter_mask(self, mask, layer, layer_index, activation_array, initialize=True):\n",
    "        \"\"\"Initialize mask for layers in CNNs with certain mask.\n",
    "        Args:\n",
    "            mask: Dict - key is a layer, value is numpy array of containing \n",
    "                0s(nodes to be removed) or 1s(nodes to keep)\n",
    "            layer: tf.keras layer\n",
    "            layer_index: Int - unique number used in naming model layers\n",
    "            activation_array: array of activations\n",
    "            initialize: Boolean - If true initializes mask to all 1s, else uses the\n",
    "                        mask corresponding to the current layer(i.e model) to drop \n",
    "                        nodes in the layer by doing element wise multiplication. \n",
    "        Returns:\n",
    "            mask: Dict - key is a layer, value is numpy array of mask\n",
    "            layer: updated tf.keras layer with mask\n",
    "        \"\"\"\n",
    "\n",
    "        layer_name = 'layer_' + str(layer_index)\n",
    "\n",
    "        if initialize:\n",
    "            mask[layer_name] = np.ones(layer.shape[3:])\n",
    "    \n",
    "        # apply mask to the layer to drop or retain nodes\n",
    "        layer = Multiply()([layer, tf.multiply(tf.ones_like(layer), mask[layer_name].reshape(1, 1, layer.shape[3]))])\n",
    "\n",
    "        activation_array.append(layer)\n",
    "        layer_index += 1\n",
    "\n",
    "        return mask, layer, activation_array, layer_index\n",
    "\n",
    "    def get_mask(self, mask, flattened_mask, post_activation_values, mask_type = 'min', percentile=0.2, drop_one=False, print_importance=False):\n",
    "        \"\"\" Updates mask after each training cycle\n",
    "        Args:\n",
    "            mask: Dict - current mask\n",
    "            flattened_mask - List of flattened masked indices per layer (1 for mask, 0 for no mask)\n",
    "            post_activation_values - list of feature maps per layer\n",
    "            mask_type: String - type of mask: min, max, random, min_layer, max_layer, random_layer\n",
    "            percentile - percentage of channels/feature maps remaining to be masked\n",
    "            print_importance - Boolean. Whether to print the importance scores per layer\n",
    "            drop_one - Boolean. Whether to drop only one channel at a time\n",
    "        Returns:\n",
    "            mask: Dict - final masks after masking percentile proportion of remaining channels\n",
    "        \"\"\"\n",
    "\n",
    "        node_importance = []\n",
    "        layer_nodes = {}\n",
    "        layer_names = list(mask.keys())\n",
    "        \n",
    "        # if only drop one, then percentile is 0\n",
    "        if drop_one:\n",
    "            percentile = 0\n",
    "        \n",
    "        for i in range(1, len(post_activation_values) - 1):\n",
    "            prev_layer = tf.reduce_sum(tf.abs(post_activation_values[i - 1]), axis=0, keepdims=True) #sum across columns. Across examples.\n",
    "            prev_layer = tf.reduce_sum(prev_layer, axis=1) #sum across rows\n",
    "\n",
    "            next_layer = tf.reduce_sum(tf.abs(post_activation_values[i + 1]), axis=0, keepdims=True)  #sum across columns. Across examples.\n",
    "            next_layer = tf.reduce_sum(next_layer, axis=1) #sum across rows\n",
    "\n",
    "            #Now sum prev and next \n",
    "            concat_prev_and_next = tf.concat([prev_layer, next_layer], axis=0)\n",
    "            sum_prev_and_next = tf.reduce_sum(concat_prev_and_next)\n",
    "\n",
    "            # add current layer nodes across examples\n",
    "            current_layer = tf.reduce_sum(tf.abs(post_activation_values[i]), axis=0) #sum across columns. Across examples\n",
    "\n",
    "            layer_node_importance = tf.multiply(current_layer, sum_prev_and_next)\n",
    "            layer_nodes[layer_names[i - 1]] = layer_node_importance\n",
    "            node_importance = np.hstack([node_importance, layer_node_importance])\n",
    "            if print_importance:\n",
    "                print('Layer node importance: ', node_importance)\n",
    "\n",
    "        # remove only those in maskindex\n",
    "        flattened_mask = np.ravel(np.where(flattened_mask == 1))\n",
    "\n",
    "        # find out the threshold node/filter value to remove\n",
    "        if len(flattened_mask) > 0:\n",
    "            # for max mask\n",
    "            if mask_type == 'max':\n",
    "                sorted_values = -np.sort(-node_importance[flattened_mask]) \n",
    "                index = int((percentile) * len(sorted_values))\n",
    "                max_index = sorted_values[index]\n",
    "            # for min or % mask\n",
    "            else:\n",
    "                sorted_values = np.sort(node_importance[flattened_mask])\n",
    "                index = int(percentile * len(sorted_values))\n",
    "                max_index = sorted_values[index]\n",
    "        \n",
    "        # Calculate the number of nodes to remove\n",
    "        num_mask = 0\n",
    "\n",
    "        for v in mask.values():\n",
    "            num_mask += np.sum(v) \n",
    "\n",
    "        total_nodes = int((percentile) * num_mask)\n",
    "\n",
    "        if drop_one:\n",
    "            total_nodes = 1\n",
    "\n",
    "        # remove at least one node\n",
    "        if (total_nodes == 0):\n",
    "            total_nodes = 1\n",
    "\n",
    "        # identify the indices to drop for random mask\n",
    "        if mask_type == 'random': # random is the only thing I don't understand in this work\n",
    "            indices = np.random.permutation(flattened_mask)\n",
    "            # take only the first total_nodes number of nodes\n",
    "            indices = indices[:total_nodes]\n",
    "            \n",
    "            dropmaskindex = {}\n",
    "            start_index = 0\n",
    "            # assign nodes/filters to drop for each layer in dropmaskindex\n",
    "            for k, v in mask.items():\n",
    "                num_mask += np.sum(v)\n",
    "                dropmaskindex[k] = indices[(indices >= start_index) & (indices < start_index + len(v))] - start_index\n",
    "                start_index += len(v)\n",
    "\n",
    "        for i, layer_node_importance in layer_nodes.items():\n",
    "            #only if there is something to drop in current mask\n",
    "            if(np.sum(mask[i])>0):\n",
    "                if mask_type == 'max': \n",
    "                    indices = np.ravel(np.where(layer_node_importance >= max_index)) #Get indices of node values >= the threshold value\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel())) #Get the mask indices\n",
    "                    indices = [j for j in indices if j in current_indices]     \n",
    "                # global random mask or layer random mask\n",
    "                elif mask_type == 'random_layer':\n",
    "                    indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                elif mask_type == 'random':\n",
    "                    indices = dropmaskindex[i]\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                # layer-wise max mask\n",
    "                elif mask_type == 'max_layer':\n",
    "                    sorted_values = -np.sort(-layer_node_importance[mask[i]==1])\n",
    "                    index = int((percentile) * len(sorted_values))\n",
    "                    maxindex = sorted_values[index]\n",
    "                    indices = np.ravel(np.where(layer_node_importance >= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "                # layer-wise min mask\n",
    "                elif mask_type == 'min_layer':\n",
    "                    sorted_values = np.sort(layer_node_importance[mask[i]==1])\n",
    "                    index = int((percentile) * len(sorted_values))\n",
    "                    max_index = sorted_values[index]\n",
    "                    indices = np.ravel(np.where(layer_node_importance <= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "                # if this is min mask or % based mask\n",
    "                else:\n",
    "                    indices = np.ravel(np.where(layer_node_importance <= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "            else:\n",
    "                #default\n",
    "                indices = np.ravel(np.where(mask[i]==1))\n",
    "\n",
    "            # shuffle the indices only if we are not dropping one node/filter\n",
    "            if (drop_one == False):\n",
    "                indices = np.random.permutation(indices)\n",
    "            \n",
    "            # Get the current mask ready for update by making a copy\n",
    "            new_mask = mask[i].ravel()\n",
    "\n",
    "            # for layer masks, total nodes dropped is by percentile of the layer of each mask\n",
    "            if(mask_type == 'random_layer') or mask_type == 'min_layer' or mask_type == 'max_layer':\n",
    "                initial_percent = np.sum(mask[i]) * 1.0 / len(mask[i].ravel())\n",
    "                total_nodes = int(initial_percent * (percentile) * len(mask[i].ravel()))\n",
    "\n",
    "                # remove at least 1 node\n",
    "                if (total_nodes == 0):\n",
    "                    total_nodes = 1\n",
    "            \n",
    "            if(len(indices) > 0):\n",
    "                # remove at most total_nodes number of nodes\n",
    "                if(len(indices) > total_nodes):\n",
    "                    indices = indices[:total_nodes]\n",
    "\n",
    "                # remove nodes\n",
    "                new_mask[indices] = 0\n",
    "\n",
    "                # update total_nodes to be removed\n",
    "                total_nodes = total_nodes - len(indices)\n",
    "            \n",
    "            # reshape to fit new mask\n",
    "            mask[i] = new_mask.reshape(mask[i].shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def get_mask_cnn(self, mask, flattened_mask, activation_model, x_train, y_train, loss_function, mask_type = 'min', percentile=0.2, drop_one=False, print_importance=False):\n",
    "        \"\"\" Updates mask after each training cycle\n",
    "        Args:\n",
    "            mask: Dict - current mask\n",
    "            flattened_mask - List of flattened masked indices per layer (1 for mask, 0 for no mask)\n",
    "            activation_model - tf.keras.Model of post activations\n",
    "            post_activation_values - list of feature maps per layer\n",
    "            mask_type: String - type of mask: min, max, random, min_layer, max_layer, random_layer\n",
    "            percentile - percentage of channels/feature maps remaining to be masked\n",
    "            print_importance - Boolean. Whether to print the importance scores per layer\n",
    "            drop_one - Boolean. Whether to drop only one channel at a time\n",
    "        Returns:\n",
    "            mask: Dict - final masks after masking percentile proportion of remaining channels\n",
    "        \"\"\"\n",
    "        \n",
    "        layer_names = list(mask.keys())\n",
    "        layer_scores = {}\n",
    "        importance = []\n",
    "\n",
    "        # if only drop one, then percentile is 0\n",
    "        if drop_one:\n",
    "            percentile = 0\n",
    "            \n",
    "        \n",
    "        #batch dataset for faster computation \n",
    "        X_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "        Y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "        train_dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "        batch_size = 256\n",
    "        batches = train_dataset.batch(batch_size)\n",
    "\n",
    "        for step, (x_train, y_train) in enumerate(batches, 1):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    activations = activation_model(x_train)\n",
    "                    loss = loss_function(y_train, activations[-1])\n",
    "                    #print(f'loss (y, predicted(output layer)): {loss}\\n')\n",
    "\n",
    "            for i in range(len(activations) - 1):\n",
    "                current_layer = tape.gradient(loss, activations[i])\n",
    "                #print(f'layer dl/d_{layer_names[i]}: {current_layer.shape}')\n",
    "\n",
    "                # sum across examples (columns)\n",
    "                current_layer_sensitivity =  tf.reduce_sum(tf.abs(current_layer), axis=0).numpy()\n",
    "                #print(f'current layer sensitivity: {current_layer_sensitivity.shape}')\n",
    "                if layer_names[i] in layer_scores:\n",
    "                    layer_scores[layer_names[i]] += current_layer_sensitivity\n",
    "                else:\n",
    "                    layer_scores[layer_names[i]] = current_layer_sensitivity\n",
    "\n",
    "        number_examples = batch_size * step\n",
    "        for layer, layer_sums in layer_scores.items():\n",
    "            #mean of each constituent gradient in a feature map across all examples \n",
    "            node_mean_across_examples = layer_sums / number_examples \n",
    "            layer_scores[layer] = tf.reduce_mean(node_mean_across_examples, axis=[0, 1])\n",
    "            importance = np.hstack([importance, layer_scores[layer]])\n",
    "        \n",
    "        if print_importance:\n",
    "            print(f'importance: {importance}')\n",
    "\n",
    "        # remove only those in maskindex\n",
    "        flattened_mask = np.ravel(np.where(flattened_mask == 1))\n",
    "\n",
    "        # find out the threshold feature map to remove\n",
    "        if len(flattened_mask) > 0:\n",
    "            # for max mask\n",
    "            if mask_type == 'max':\n",
    "                sorted_values = -np.sort(-importance[flattened_mask]) \n",
    "                index = int((percentile) * len(sorted_values))\n",
    "                max_index = sorted_values[index]\n",
    "            # for min or % mask\n",
    "            else:\n",
    "                sorted_values = np.sort(importance[flattened_mask])\n",
    "                index = int(percentile * len(sorted_values))\n",
    "                max_index = sorted_values[index]\n",
    "        \n",
    "        # Calculate the number of nodes to remove\n",
    "        num_mask = 0\n",
    "\n",
    "        for v in mask.values():\n",
    "            num_mask += np.sum(v) \n",
    "\n",
    "        total_nodes = int((percentile) * num_mask)\n",
    "\n",
    "        if drop_one:\n",
    "            total_nodes = 1\n",
    "\n",
    "        # remove at least one node\n",
    "        if (total_nodes == 0):\n",
    "            total_nodes = 1\n",
    "\n",
    "        # identify the indices to drop for random mask\n",
    "        if mask_type == 'random': # random is the only thing I don't understand in this work\n",
    "            indices = np.random.permutation(flattened_mask)\n",
    "            # take only the first total_nodes number of nodes\n",
    "            indices = indices[:total_nodes]\n",
    "            \n",
    "            dropmaskindex = {}\n",
    "            start_index = 0\n",
    "            # assign nodes/filters to drop for each layer in dropmaskindex\n",
    "            for k, v in mask.items():\n",
    "                num_mask += np.sum(v)\n",
    "                dropmaskindex[k] = indices[(indices >= start_index) & (indices < start_index + len(v))] - start_index\n",
    "                start_index += len(v)\n",
    "\n",
    "        for i, layer_importance in layer_scores.items():\n",
    "            #only if there is something to drop in current mask\n",
    "            if(np.sum(mask[i])>0):\n",
    "                if mask_type == 'max': \n",
    "                    indices = np.ravel(np.where(layer_importance >= max_index)) #Get indices of node values >= the threshold value\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel())) #Get the mask indices\n",
    "                    indices = [j for j in indices if j in current_indices]     \n",
    "                # global random mask or layer random mask\n",
    "                elif mask_type == 'random_layer':\n",
    "                    indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                elif mask_type == 'random':\n",
    "                    indices = dropmaskindex[i]\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                # layer-wise max mask\n",
    "                elif mask_type == 'max_layer':\n",
    "                    sorted_values = -np.sort(-layer_importance[mask[i]==1])\n",
    "                    index = int((percentile) * len(sorted_values))\n",
    "                    max_index = sorted_values[index]\n",
    "                    indices = np.ravel(np.where(layer_importance >= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "                # layer-wise min mask\n",
    "                elif mask_type == 'min_layer':\n",
    "                    sorted_values = np.sort(layer_importance[mask[i]==1])\n",
    "                    index = int((percentile) * len(sorted_values))\n",
    "                    max_index = sorted_values[index]\n",
    "                    indices = np.ravel(np.where(layer_importance <= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "                # if this is min mask or % based mask\n",
    "                else:\n",
    "                    indices = np.ravel(np.where(layer_importance <= max_index))\n",
    "                    current_indices = np.ravel(np.where(mask[i].ravel()))\n",
    "                    indices = [j for j in indices if j in current_indices]\n",
    "            else:\n",
    "                #default\n",
    "                indices = np.ravel(np.where(mask[i]==1))\n",
    "\n",
    "            # shuffle the indices only if we are not dropping one filter\n",
    "            if (drop_one == False):\n",
    "                indices = np.random.permutation(indices)\n",
    "            \n",
    "            # Get the current mask ready for update by making a copy\n",
    "            new_mask = mask[i].ravel()\n",
    "\n",
    "            # for layer masks, total nodes dropped is by percentile of the layer of each mask\n",
    "            if(mask_type == 'random_layer') or mask_type == 'min_layer' or mask_type == 'max_layer':\n",
    "                initial_percent = np.sum(mask[i]) * 1.0 / len(mask[i].ravel())\n",
    "                total_nodes = int(initial_percent * (percentile) * len(mask[i].ravel()))\n",
    "\n",
    "                # remove at least 1 node\n",
    "                if (total_nodes == 0):\n",
    "                    total_nodes = 1\n",
    "            \n",
    "            if(len(indices) > 0):\n",
    "                # remove at most total_nodes number of nodes\n",
    "                if(len(indices) > total_nodes):\n",
    "                    indices = indices[:total_nodes]\n",
    "\n",
    "                # remove nodes\n",
    "                new_mask[indices] = 0\n",
    "\n",
    "                # update total_nodes to be removed\n",
    "                total_nodes = total_nodes - len(indices)\n",
    "            \n",
    "            # reshape to fit new mask\n",
    "            mask[i] = new_mask.reshape(mask[i].shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def generate_model(self, experiment_name, opt = SGD(lr = 0.1), loss = 'sparse_categorical_crossentropy',\n",
    "                        metrics = ['accuracy'], callbacks = [EarlyStopping(monitor='val_loss', mode = 'min', verbose=0, patience=5)], \n",
    "                        num_epochs = 100, percentile = 0.2, verbose = 0, num_trials = 15, print_value = True, \n",
    "                        print_mask = False, print_importance = False, cnn=True):\n",
    "        \"\"\" Function to evaluate the model against various metrics\n",
    "        Inputs:\n",
    "        opt - Keras optimizer\n",
    "        loss - Keras loss\n",
    "        metrics - Keras metrics\n",
    "        callbacks - Keras callbacks\n",
    "        num_epochs - Number of epochs for each training cycle\n",
    "        percentile - Percentile to drop the nodes\n",
    "        verbose - Keras verbose option\n",
    "        num_trials - Number of different experiments to run, each with different random seed\n",
    "        print_value  - Boolean. Whether to print the accuracies and losses for each pruning percentage\n",
    "        print_mask - Boolean. Whether to print the mask values\n",
    "        print_activation - Boolean. Whether to print the node/filter's activation values\n",
    "        \n",
    "        Output:\n",
    "        caches - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        percent_removed ={}\n",
    "        train_accuracies = {}\n",
    "        valid_accuracies = {}\n",
    "        test_accuracies = {}\n",
    "        train_losses = {}\n",
    "        valid_losses ={}\n",
    "        test_losses = {}\n",
    "        early_stopping = {}\n",
    "        oracle_comparison = {}\n",
    "        masktypes = ['min', 'max', 'random', 'min_layer', 'max_layer', 'random_layer']\n",
    "        \n",
    "        for masktype in masktypes:\n",
    "            percent_removed[masktype] = []\n",
    "            train_accuracies[masktype] = []\n",
    "            valid_accuracies[masktype] = []\n",
    "            test_accuracies[masktype] = []\n",
    "            train_losses[masktype] = []\n",
    "            valid_losses[masktype] = []\n",
    "            test_losses[masktype] = []\n",
    "            early_stopping[masktype] = []\n",
    "            \n",
    "        # do for num_trials number of random seeds\n",
    "        for random_seed in range(num_trials):\n",
    "            print('>>>     Random seed number:', random_seed)\n",
    "\n",
    "            np.random.seed(random_seed)\n",
    "            tf.random.set_seed(random_seed)    \n",
    "            \n",
    "            # Initialize the model\n",
    "            mask, model, activation_model = self.initialize_model()\n",
    "            # model.summary()\n",
    "            model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "            # save original weights\n",
    "            weights_initial = model.get_weights()\n",
    "        \n",
    "        # do for all mask types\n",
    "            for masktype in masktypes:\n",
    "                print('\\n>>> Currently doing', masktype, 'mask <<<')\n",
    "            \n",
    "                mask=self.reset_mask(mask)\n",
    "                percent = self.percent_mask(mask)\n",
    "                \n",
    "                while percent > 0.1:\n",
    "                    # Initialize the model with the new mask\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    np.random.seed(random_seed)\n",
    "                    tf.random.set_seed(random_seed)  \n",
    "                    \n",
    "                    _, model, activation_model = self.initialize_model(mask)\n",
    "\n",
    "                    #Initialize to original weights\n",
    "                    model.set_weights(weights_initial)\n",
    "                    model.compile(optimizer = opt, loss = loss, metrics = metrics)  \n",
    "\n",
    "                    history = model.fit(self.x_train, self.y_train, epochs = num_epochs, validation_data = (self.x_val, self.y_val), callbacks = callbacks, shuffle = False, verbose = verbose)\n",
    "                    results = model.evaluate(self.x_test, self.y_test, verbose = 0)\n",
    "\n",
    "                    percent = self.percent_mask(mask)\n",
    "                    train_accuracy = history.history['accuracy'][-1]\n",
    "                    valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                    test_accuracy = results[1]\n",
    "                    train_loss = history.history['loss'][-1]\n",
    "                    valid_loss = history.history['val_loss'][-1]\n",
    "                    test_loss = results[0]\n",
    "                    early = len(history.history['accuracy'])\n",
    "\n",
    "                    # Append the values for accuracy and loss\n",
    "                    percent_removed[masktype].append(percent)\n",
    "                    train_accuracies[masktype].append(train_accuracy)\n",
    "                    valid_accuracies[masktype].append(valid_accuracy)\n",
    "                    test_accuracies[masktype].append(test_accuracy)\n",
    "                    train_losses[masktype].append(train_loss)\n",
    "                    valid_losses[masktype].append(valid_loss)\n",
    "                    test_losses[masktype].append(test_loss)\n",
    "                    early_stopping[masktype].append(early)\n",
    "\n",
    "                    if print_value:\n",
    "                        print('Percentage remaining', percent, end = ' ')\n",
    "                        print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                        if print_mask:\n",
    "                            print('Mask:', mask)\n",
    "                        print('Train Acc:', train_accuracy, end = ' ')\n",
    "                        print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                        print('Test Acc:', test_accuracy)\n",
    "                        print('Train Loss:', train_loss, end = ' ')\n",
    "                        print('Val loss:', valid_loss, end = ' ')\n",
    "                        print('Test Loss:', test_loss)\n",
    "                        print('Early stopping iteration:', early)\n",
    "\n",
    "                    # Remove nodes for next iteration based on metric\n",
    "                    post_activation_values = activation_model.predict(self.x_train)\n",
    "                    loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                    flattened_mask = self.flatten_mask(mask)\n",
    "                    if cnn:\n",
    "                        mask = self.get_mask_cnn(mask, flattened_mask, activation_model, self.x_train, self.y_train, loss_function, mask_type=masktype, percentile=percentile, print_importance=print_importance)\n",
    "                    else:\n",
    "                        mask = self.get_mask(mask, flattened_mask, post_activation_values, mask_type=masktype, percentile=percentile, print_importance=print_importance)\n",
    "                    \n",
    "\n",
    "            cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "            self.print_graph(cache, 'evaluate'+str(random_seed), experiment_name, num_trials = random_seed+1)\n",
    "\n",
    "        cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "        return cache\n",
    "\n",
    "    def compare_random(self, experiment_name, opt = SGD(lr = 0.1),\n",
    "                        loss = 'sparse_categorical_crossentropy',\n",
    "                        callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)], \n",
    "                        metrics = ['accuracy'], num_epochs = 100, percentile = 0.2, verbose = 0, num_trials = 15, \n",
    "                        print_value = True, print_mask = False, print_importance = False, cnn=True):\n",
    "        \"\"\" Function to compare metrics with randominit \n",
    "        Inputs:\n",
    "        opt - Keras optimizer\n",
    "        loss - Keras loss\n",
    "        callbacks - Keras callbacks\n",
    "        metrics - Keras metrics\n",
    "        num_epochs - Number of epochs for each training cycle\n",
    "        percentile - Percentile to drop the nodes\n",
    "        verbose - Keras verbose option\n",
    "        num_trials - Number of different experiments to run, each with different random seed\n",
    "        print_value  - Boolean. Whether to print the accuracies and losses for each pruning percentage\n",
    "        print_mask - Boolean. Whether to print the mask values\n",
    "        print_importance - Boolean. Whether to print the node/filter's activation values\n",
    "        \n",
    "        Output:\n",
    "        caches - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        percent_removed ={}\n",
    "        train_accuracies = {}\n",
    "        valid_accuracies = {}\n",
    "        test_accuracies = {}\n",
    "        train_losses = {}\n",
    "        valid_losses ={}\n",
    "        test_losses = {}\n",
    "        early_stopping = {}\n",
    "        oracle_comparison = {}\n",
    "        masktypes = ['min', 'randominit']\n",
    "        \n",
    "        for masktype in masktypes:\n",
    "            percent_removed[masktype] = []\n",
    "            train_accuracies[masktype] = []\n",
    "            valid_accuracies[masktype] = []\n",
    "            test_accuracies[masktype] = []\n",
    "            train_losses[masktype] = []\n",
    "            valid_losses[masktype] = []\n",
    "            test_losses[masktype] = []\n",
    "            early_stopping[masktype] = []\n",
    "            \n",
    "        # do for num_trials number of random seeds\n",
    "        for random_seed in range(num_trials):\n",
    "            print('>>>     Random seed number:', random_seed)\n",
    "\n",
    "            np.random.seed(random_seed)\n",
    "            tf.random.set_seed(random_seed)   \n",
    "            \n",
    "            # Initialize the model\n",
    "            mask, model, activation_model = self.initialize_model()\n",
    "            # model.summary()\n",
    "            model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "            \n",
    "            # save original weights\n",
    "            weights_initial = model.get_weights()\n",
    "\n",
    "            mask = self.reset_mask(mask)\n",
    "            percent = self.percent_mask(mask)\n",
    "            \n",
    "            while percent > 0.1:\n",
    "                print('\\n>>> Currently doing min mask <<<')\n",
    "                tf.keras.backend.clear_session()\n",
    "                np.random.seed(random_seed)\n",
    "                tf.random.set_seed(random_seed)  \n",
    "                \n",
    "                # Initialize the model with the new mask\n",
    "                _, model, activation_model = self.initialize_model(mask)\n",
    "\n",
    "                #Initialize to original weights\n",
    "                model.set_weights(weights_initial)\n",
    "                model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "\n",
    "                history = model.fit(self.x_train, self.y_train, epochs = num_epochs, validation_data = (self.x_val, self.y_val), shuffle = False, callbacks = callbacks, verbose = verbose)\n",
    "                results = model.evaluate(self.x_test, self.y_test, verbose = 0)\n",
    "\n",
    "                percent = self.percent_mask(mask)\n",
    "                train_accuracy = history.history['accuracy'][-1]\n",
    "                valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                test_accuracy = results[1]\n",
    "                train_loss = history.history['loss'][-1]\n",
    "                valid_loss = history.history['val_loss'][-1]\n",
    "                test_loss = results[0]\n",
    "                early = len(history.history['accuracy'])\n",
    "\n",
    "                # Append the values for accuracy and loss\n",
    "                percent_removed['min'].append(percent)\n",
    "                train_accuracies['min'].append(train_accuracy)\n",
    "                valid_accuracies['min'].append(valid_accuracy)\n",
    "                test_accuracies['min'].append(test_accuracy)\n",
    "                train_losses['min'].append(train_loss)\n",
    "                valid_losses['min'].append(valid_loss)\n",
    "                test_losses['min'].append(test_loss)\n",
    "                early_stopping['min'].append(early)\n",
    "\n",
    "                if print_value:\n",
    "                    print('Percentage remaining', percent, end = ' ')\n",
    "                    print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                    if print_mask:\n",
    "                        print('Mask:', mask)\n",
    "                    print('Train Acc:', train_accuracy, end = ' ')\n",
    "                    print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                    print('Test Acc:', test_accuracy)\n",
    "                    print('Train Loss:', train_loss, end = ' ')\n",
    "                    print('Val loss:', valid_loss, end = ' ')\n",
    "                    print('Test Loss:', test_loss)\n",
    "                    print('Early stopping iteration:', early)\n",
    "                    \n",
    "                # Remove nodes for next iteration based on metric\n",
    "                post_activation_values = activation_model.predict(self.x_train)\n",
    "                flattened_mask = self.flatten_mask(mask)\n",
    "\n",
    "                # random init mask\n",
    "                print('\\n>>> Currently doing randominit mask <<<')\n",
    "                # Initialize the model with the new mask with random seed\n",
    "                tf.keras.backend.clear_session()\n",
    "                np.random.seed(random_seed+20)\n",
    "                tf.random.set_seed(random_seed+20)    \n",
    "                _, model, activation_model = self.initialize_model(mask)\n",
    "\n",
    "                model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "\n",
    "                history = model.fit(self.x_train, self.y_train, epochs = num_epochs, validation_data = (self.x_val, self.y_val), shuffle = False, callbacks = callbacks, verbose = verbose)\n",
    "                results = model.evaluate(self.x_test, self.y_test, verbose = 0)\n",
    "\n",
    "                percent = self.percent_mask(mask)\n",
    "                train_accuracy = history.history['accuracy'][-1]\n",
    "                valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                test_accuracy = results[1]\n",
    "                train_loss = history.history['loss'][-1]\n",
    "                valid_loss = history.history['val_loss'][-1]\n",
    "                test_loss = results[0]\n",
    "                early = len(history.history['accuracy'])\n",
    "\n",
    "                # Append the values for accuracy and loss\n",
    "                percent_removed['randominit'].append(percent)\n",
    "                train_accuracies['randominit'].append(train_accuracy)\n",
    "                valid_accuracies['randominit'].append(valid_accuracy)\n",
    "                test_accuracies['randominit'].append(test_accuracy)\n",
    "                train_losses['randominit'].append(train_loss)\n",
    "                valid_losses['randominit'].append(valid_loss)\n",
    "                test_losses['randominit'].append(test_loss)\n",
    "                early_stopping['randominit'].append(early)\n",
    "\n",
    "                if print_value:\n",
    "                    print('Percentage remaining', percent, end = ' ')\n",
    "                    print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                    if print_mask:\n",
    "                        print('Mask:', mask)\n",
    "                    print('Train Acc:', train_accuracy, end = ' ')\n",
    "                    print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                    print('Test Acc:', test_accuracy)\n",
    "                    print('Train Loss:', train_loss, end = ' ')\n",
    "                    print('Val loss:', valid_loss, end = ' ')\n",
    "                    print('Test Loss:', test_loss)\n",
    "                    print('Early stopping iteration:', early)\n",
    "                \n",
    "                loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                # get new mask\n",
    "                if cnn:\n",
    "                    mask = self.get_mask_cnn(mask, flattened_mask, activation_model, self.x_train, self.y_train, loss_function, mask_type='min', percentile=percentile, print_importance=print_importance)\n",
    "                else:\n",
    "                    mask = self.get_mask(mask, flattened_mask, post_activation_values, mask_type='min', percentile=percentile, print_importance=print_importance)\n",
    "    \n",
    "\n",
    "            cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "            self.print_graph(cache, 'randominit'+str(random_seed), experiment_name, num_trials = random_seed+1)\n",
    "\n",
    "        cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "        return cache \n",
    "\n",
    "    def oracle_model(self, experiment_name, opt = SGD(lr = 0.1), loss = 'sparse_categorical_crossentropy',\n",
    "                    metrics = ['accuracy'],\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', mode = 'min', verbose=0, patience=5)], \n",
    "                    num_epochs = 100, percentile = 0.2, verbose = 0, num_trials = 15, print_value = True, \n",
    "                    print_mask = False, print_importance = False, cnn=True):\n",
    "        \n",
    "        \"\"\" Function to evaluate the model against the oracle\n",
    "        Inputs:\n",
    "        opt - Keras optimizer\n",
    "        loss - Keras loss\n",
    "        metrics - Keras metrics\n",
    "        callbacks - Keras callbacks\n",
    "        num_epochs - Number of epochs for each training cycle\n",
    "        percentile - Percentile to drop the nodes\n",
    "        verbose - Keras verbose option\n",
    "        num_trials - Number of different experiments to run, each with different random seed\n",
    "        print_value  - Boolean. Whether to print the accuracies and losses for each pruning percentage\n",
    "        print_mask - Boolean. Whether to print the mask values\n",
    "        print_importance - Boolean. Whether to print the node/filter's activation values\n",
    "        \n",
    "        Output:\n",
    "        caches - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        percent_removed ={}\n",
    "        train_accuracies = {}\n",
    "        valid_accuracies = {}\n",
    "        test_accuracies = {}\n",
    "        train_losses = {}\n",
    "        valid_losses ={}\n",
    "        test_losses = {}\n",
    "        early_stopping = {}\n",
    "        oracle_comparison = {}\n",
    "        oracle_masks = []\n",
    "        masktypes = ['oracle', 'min', 'max', 'random_layer']\n",
    "        \n",
    "        for masktype in masktypes:\n",
    "            percent_removed[masktype] = []\n",
    "            train_accuracies[masktype] = []\n",
    "            valid_accuracies[masktype] = []\n",
    "            test_accuracies[masktype] = []\n",
    "            train_losses[masktype] = []\n",
    "            valid_losses[masktype] = []\n",
    "            test_losses[masktype] = []\n",
    "            early_stopping[masktype] = []\n",
    "            oracle_comparison[masktype] = []\n",
    "            \n",
    "        # do for num_trials number of random seeds\n",
    "        for random_seed in range(num_trials):\n",
    "            print('>>>     Random seed number:', random_seed)\n",
    "\n",
    "            np.random.seed(random_seed)\n",
    "            tf.random.set_seed(random_seed)    \n",
    "            \n",
    "            # Initialize the model\n",
    "            mask, model, activation_model = self.initialize_model()\n",
    "            # model.summary()\n",
    "            model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "            # save original weights\n",
    "            weights_initial = model.get_weights()\n",
    "            \n",
    "            # reset oracle masks\n",
    "            oracle_masks = []\n",
    "                    \n",
    "            # do for all mask types\n",
    "            for masktype in masktypes:\n",
    "                print('\\n>>> Currently doing', masktype, 'mask <<<')\n",
    "            \n",
    "                mask=self.reset_mask(mask)\n",
    "                percent = self.percent_mask(mask)\n",
    "                iterationnum = 0\n",
    "                \n",
    "                while percent > 0.1:\n",
    "                    # Initialize the model with the new mask\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    np.random.seed(random_seed)\n",
    "                    tf.random.set_seed(random_seed)  \n",
    "                    \n",
    "                    _, model, activation_model = self.initialize_model(mask)\n",
    "                    \n",
    "                    #Initialize to original weights\n",
    "                    model.set_weights(weights_initial)\n",
    "                    model.compile(optimizer = opt, loss = loss, metrics = metrics)  \n",
    "\n",
    "                    history = model.fit(self.x_train, self.y_train, epochs = num_epochs, validation_data = (self.x_val, self.y_val), callbacks = callbacks, shuffle = False, verbose = verbose)\n",
    "                    results = model.evaluate(self.x_test, self.y_test, verbose = 0)\n",
    "                    \n",
    "                    weights_after_training = model.get_weights()\n",
    "\n",
    "                    percent = self.percent_mask(mask)\n",
    "                    train_accuracy = history.history['accuracy'][-1]\n",
    "                    valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                    test_accuracy = results[1]\n",
    "                    train_loss = history.history['loss'][-1]\n",
    "                    valid_loss = history.history['val_loss'][-1]\n",
    "                    test_loss = results[0]\n",
    "                    early = len(history.history['accuracy'])\n",
    "\n",
    "                    # Append the values for accuracy and loss\n",
    "                    percent_removed[masktype].append(percent)\n",
    "                    train_accuracies[masktype].append(train_accuracy)\n",
    "                    valid_accuracies[masktype].append(valid_accuracy)\n",
    "                    test_accuracies[masktype].append(test_accuracy)\n",
    "                    train_losses[masktype].append(train_loss)\n",
    "                    valid_losses[masktype].append(valid_loss)\n",
    "                    test_losses[masktype].append(test_loss)\n",
    "                    early_stopping[masktype].append(early)\n",
    "                    \n",
    "                    # Compare mask with oracle if this is iteration 1 and above (when there is a mask to compare)\n",
    "                    if iterationnum == 0:\n",
    "                        compare = 1\n",
    "                    else:\n",
    "                        compare = self.compare_mask(mask, oracle_masks[iterationnum-1])\n",
    "                    oracle_comparison[masktype].append(compare)\n",
    "\n",
    "                    if print_value:\n",
    "                        print('Percentage remaining', percent, end = ' ')\n",
    "                        print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                        if print_mask:\n",
    "                            print('Mask:', mask)\n",
    "                        print('Train Acc:', train_accuracy, end = ' ')\n",
    "                        print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                        print('Test Acc:', test_accuracy)\n",
    "                        print('Train Loss:', train_loss, end = ' ')\n",
    "                        print('Val loss:', valid_loss, end = ' ')\n",
    "                        print('Test Loss:', test_loss)\n",
    "                        print('Early stopping iteration:', early)\n",
    "                        print('Similaritiy with oracle:', compare)\n",
    "\n",
    "                    # Remove nodes for next iteration for oracle by doing brute force comparison\n",
    "                    if masktype == 'oracle':\n",
    "                        bestloss = 1000000\n",
    "                        bestmask = {}\n",
    "                        \n",
    "                        # Remove one existing node and compare accuracy\n",
    "                        for v in mask.values():\n",
    "                            for index in range(len(v)):\n",
    "                                # remove mask index only if it is non-zero\n",
    "                                if v[index] != 0:\n",
    "                                    \n",
    "                                    # take out mask index\n",
    "                                    v[index] = 0\n",
    "                                    \n",
    "                                    # do the mask evaluation\n",
    "                                    # Initialize the model with the new mask\n",
    "                                    _, model, activation_model = self.initialize_model(mask)\n",
    "\n",
    "                                    #Initialize the weights to after training weights\n",
    "                                    model.set_weights(weights_after_training)\n",
    "                                    model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "                                    # evaluate model using training data only\n",
    "                                    results = model.evaluate(self.x_train, self.y_train, verbose = 0)\n",
    "                                    \n",
    "                                    # update accuracy and mask for best mask\n",
    "                                    if(results[0] < bestloss):\n",
    "                                        bestloss = results[0]\n",
    "                                        bestmask = copy.deepcopy(mask)\n",
    "                                            \n",
    "                                    # put back mask index\n",
    "                                    v[index] = 1\n",
    "                            \n",
    "                        # store bestmask\n",
    "                        mask = copy.deepcopy(bestmask)\n",
    "                        oracle_masks.append(mask)\n",
    "                        \n",
    "                        if print_value:\n",
    "                            print('Final mask selected:', mask)\n",
    "                        oracle_comparison[masktype].append(self.compare_mask(mask, oracle_masks[iterationnum]))\n",
    "\n",
    "                    else:\n",
    "                        # Remove nodes for next iteration based on metric\n",
    "                        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                        post_activation_values = activation_model.predict(self.x_train)\n",
    "                        flattened_mask = self.flatten_mask(mask)\n",
    "                        if cnn:\n",
    "                            mask = self.get_mask_cnn(mask, flattened_mask, activation_model, self.x_train, self.y_train, loss_function, percentile=percentile, drop_one=True, print_importance=print_importance)\n",
    "                        else:\n",
    "                            mask = self.get_mask(mask, flattened_mask, post_activation_values, percentile=percentile, drop_one=True, print_importance=print_importance)\n",
    "            \n",
    "                        oracle_comparison[masktype].append(self.compare_mask(mask, oracle_masks[iterationnum]))\n",
    "                    \n",
    "                    # Increment iteration number\n",
    "                    iterationnum = iterationnum + 1\n",
    "                    \n",
    "            cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "            self.print_graph(cache, 'oracle'+str(random_seed), experiment_name, num_trials = random_seed+1)\n",
    "\n",
    "        cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "        return cache\n",
    "\n",
    "    def compare_percent_mask(self, experiment_name, opt = SGD(lr = 0.1), loss = 'sparse_categorical_crossentropy',\n",
    "                            metrics = ['accuracy'],\n",
    "                            callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)], \n",
    "                            num_epochs = 100, percentile = 0.2, verbose = 0, num_trials = 15, print_value = True, \n",
    "                            print_mask = False, print_importance = False, cnn=True):\n",
    "        \"\"\" Function to compare the effect of pruning proportions\n",
    "        Inputs:\n",
    "        opt - Keras optimizer\n",
    "        loss - Keras loss\n",
    "        metrics - Keras metrics\n",
    "        callbacks - Keras callbacks\n",
    "        num_epochs - Number of epochs for each training cycle\n",
    "        percentile - Percentile to drop the nodes\n",
    "        verbose - Keras verbose option\n",
    "        num_trials - Number of different experiments to run, each with different random seed\n",
    "        print_value  - Boolean. Whether to print the accuracies and losses for each pruning percentage\n",
    "        print_mask - Boolean. Whether to print the mask values\n",
    "        print_importance - Boolean. Whether to print the node/filter's activation values\n",
    "        \n",
    "        Output:\n",
    "        caches - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize variables\n",
    "        percent_removed ={}\n",
    "        train_accuracies = {}\n",
    "        valid_accuracies = {}\n",
    "        test_accuracies = {}\n",
    "        train_losses = {}\n",
    "        valid_losses ={}\n",
    "        test_losses = {}\n",
    "        early_stopping = {}\n",
    "        oracle_comparison = {}\n",
    "        masktypes = ['0.2', '0.3', '0.4', '0.5', '0.9']\n",
    "        \n",
    "        for masktype in masktypes:\n",
    "            percent_removed[masktype] = []\n",
    "            train_accuracies[masktype] = []\n",
    "            valid_accuracies[masktype] = []\n",
    "            test_accuracies[masktype] = []\n",
    "            train_losses[masktype] = []\n",
    "            valid_losses[masktype] = []\n",
    "            test_losses[masktype] = []\n",
    "            early_stopping[masktype] = []\n",
    "            \n",
    "        # do for num_trials number of random seeds\n",
    "        for random_seed in range(num_trials):\n",
    "            print('>>>     Random seed number:', random_seed)\n",
    "\n",
    "            np.random.seed(random_seed)\n",
    "            tf.random.set_seed(random_seed)    \n",
    "            \n",
    "            # Initialize the model\n",
    "            mask, model, activation_model = self.initialize_model()\n",
    "            # model.summary()\n",
    "            model.compile(optimizer = opt, loss = loss, metrics = metrics)\n",
    "            # save original weights\n",
    "            weights_initial = model.get_weights()\n",
    "        \n",
    "        # do for all mask types\n",
    "            for masktype in masktypes:\n",
    "                print('\\n>>> Currently doing', masktype, 'mask <<<')\n",
    "            \n",
    "                mask=self.reset_mask(mask)\n",
    "                percent = self.percent_mask(mask)\n",
    "                \n",
    "                while percent > 0.1:\n",
    "                    # Initialize the model with the new mask\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    np.random.seed(random_seed)\n",
    "                    tf.random.set_seed(random_seed)  \n",
    "                    \n",
    "                    _, model, activation_model = self.initialize_model(mask)\n",
    "\n",
    "                    #Initialize to original weights\n",
    "                    model.set_weights(weights_initial)\n",
    "                    model.compile(optimizer = opt, loss = loss, metrics = metrics)  \n",
    "\n",
    "                    history = model.fit(self.x_train, self.y_train, epochs = num_epochs, validation_data = (self.x_val, self.y_val), callbacks = callbacks, shuffle = False, verbose = verbose)\n",
    "                    results = model.evaluate(self.x_test, self.y_test, verbose = 0)\n",
    "\n",
    "                    percent = self.percent_mask(mask)\n",
    "                    train_accuracy = history.history['accuracy'][-1]\n",
    "                    valid_accuracy = history.history['val_accuracy'][-1]\n",
    "                    test_accuracy = results[1]\n",
    "                    train_loss = history.history['loss'][-1]\n",
    "                    valid_loss = history.history['val_loss'][-1]\n",
    "                    test_loss = results[0]\n",
    "                    early = len(history.history['accuracy'])\n",
    "\n",
    "                    # Append the values for accuracy and loss\n",
    "                    percent_removed[masktype].append(percent)\n",
    "                    train_accuracies[masktype].append(train_accuracy)\n",
    "                    valid_accuracies[masktype].append(valid_accuracy)\n",
    "                    test_accuracies[masktype].append(test_accuracy)\n",
    "                    train_losses[masktype].append(train_loss)\n",
    "                    valid_losses[masktype].append(valid_loss)\n",
    "                    test_losses[masktype].append(test_loss)\n",
    "                    early_stopping[masktype].append(early)\n",
    "\n",
    "                    if print_value:\n",
    "                        print('Percentage remaining', percent, end = ' ')\n",
    "                        print('Layer nodes:', [np.sum(mask[i]) for i in mask.keys()], end = ' ')\n",
    "                        if print_mask:\n",
    "                            print('Mask:', mask)\n",
    "                        print('Train Acc:', train_accuracy, end = ' ')\n",
    "                        print('Val Acc:', valid_accuracy, end = ' ')\n",
    "                        print('Test Acc:', test_accuracy)\n",
    "                        print('Train Loss:', train_loss, end = ' ')\n",
    "                        print('Val loss:', valid_loss, end = ' ')\n",
    "                        print('Test Loss:', test_loss)\n",
    "                        print('Early stopping iteration:', early)\n",
    "                        \n",
    "                    # get the percentile from masktype\n",
    "                    if masktype == '0.1': \n",
    "                        percentile = 0.1\n",
    "                    elif masktype == '0.2': \n",
    "                        percentile = 0.2\n",
    "                    elif masktype == '0.3': \n",
    "                        percentile = 0.3\n",
    "                    elif masktype == '0.4': \n",
    "                        percentile = 0.4\n",
    "                    elif masktype == '0.5': \n",
    "                        percentile = 0.5\n",
    "                    elif masktype == '0.9': \n",
    "                        percentile = 0.9\n",
    "\n",
    "                    # Remove nodes for next iteration based on metric\n",
    "                    loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                    post_activation_values = activation_model.predict(self.x_train)\n",
    "                    flattened_mask = self.flatten_mask(mask)\n",
    "                    if cnn:\n",
    "                        mask = self.get_mask_cnn(mask, flattened_mask, activation_model, self.x_train, self.y_train, loss_function, mask_type='min', percentile=percentile, print_importance=print_importance)\n",
    "                    else:\n",
    "                        mask = self.get_mask(mask, flattened_mask, post_activation_values, mask_type='min', percentile=percentile, print_importance=print_importance)\n",
    "\n",
    "                    \n",
    "            cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "            self.print_graph(cache, 'comparepercentmask'+str(random_seed), experiment_name, num_trials = random_seed+1)\n",
    "                    \n",
    "        cache = (percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison)\n",
    "        return cache\n",
    "\n",
    "    def print_graph(self, cache, name, experiment_name, num_trials = 15, oracle = False):\n",
    "        \"\"\"Function to print graph\n",
    "        Input: \n",
    "        cache - Cache containing accuracies, losses, early stopping iteration and oracle comparison\n",
    "        name - name of model which is run\n",
    "        num_trials - number of experiments conducted\n",
    "        oracle - Boolean. Whether the oracle_comparison graph is required\n",
    "        \n",
    "        Outputs:\n",
    "        Graphs for training, validation, test accuracy, early stopping iteration, oracle comparison (optional)\"\"\"\n",
    "        \n",
    "        # unpack caches\n",
    "        percent_removed, train_accuracies, valid_accuracies, test_accuracies, train_losses, valid_losses, test_losses, early_stopping, oracle_comparison = cache\n",
    "        # sort masktypes by alphabetical order\n",
    "        masktypes = sorted(percent_removed.keys())\n",
    "        \n",
    "        # Save result of each experiment in a folder\n",
    "        if not os.path.exists(experiment_name):\n",
    "            os.mkdir(experiment_name)\n",
    "\n",
    "        # Set colors\n",
    "        colors = {}\n",
    "        colors['min'] = 'b'\n",
    "        colors['max'] = 'r'\n",
    "        colors['random'] ='g'\n",
    "        colors['random_layer'] = 'y'\n",
    "        colors['min_layer'] = 'c'\n",
    "        colors['max_layer'] = 'm'\n",
    "        colors['randominit'] = 'y'\n",
    "        colors['oracle'] = 'm'\n",
    "        \n",
    "        # colors for percentage mask\n",
    "        colors['0.1'] = 'b'\n",
    "        colors['0.2'] = 'r'\n",
    "        colors['0.3'] = 'g'\n",
    "        colors['0.4'] = 'y'\n",
    "        colors['0.5'] = 'c'\n",
    "        colors['0.9'] = 'm'\n",
    "        \n",
    "        # format for various masks\n",
    "        fmt = {}\n",
    "        fmt['min'] = '-'\n",
    "        fmt['max'] = '-'\n",
    "        fmt['random'] ='--'\n",
    "        fmt['random_layer'] = '--'\n",
    "        fmt['min_layer'] = '-'\n",
    "        fmt['max_layer'] = '-'\n",
    "        fmt['randominit'] = '--'\n",
    "        fmt['minfast'] = '-'\n",
    "        fmt['oracle'] = '-'\n",
    "        \n",
    "        fmt['0.1'] = '-'\n",
    "        fmt['0.2'] = '-'\n",
    "        fmt['0.3'] = '-'\n",
    "        fmt['0.4'] = '-'\n",
    "        fmt['0.5'] = '-'\n",
    "        fmt['0.9'] = '-'\n",
    "        \n",
    "        #import matplotlib.font_manager\n",
    "        #from matplotlib import rc, rcParams\n",
    "        rc('font', family = 'STIXGeneral')\n",
    "        rc('xtick', labelsize=10) \n",
    "        rcParams.update({'figure.autolayout': True})\n",
    "        rcParams.update({'font.size': 14})\n",
    "        \n",
    "        # Plot figures for training accuracy\n",
    "        plt.figure()\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xscale('log')\n",
    "        plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "        for masktype in masktypes:     \n",
    "            length = len(percent_removed[masktype])//num_trials\n",
    "            mean = []\n",
    "            std = []\n",
    "            for i in range(length):\n",
    "                mean.append(np.mean(train_accuracies[masktype][i::length]))\n",
    "                std.append(1.96*np.std(train_accuracies[masktype][i::length])/np.sqrt(num_trials))\n",
    "            plt.errorbar(percent_removed[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)        \n",
    "        plt.ylabel('Training accuracy')\n",
    "        plt.xlabel('Proportion of nodes/feature maps remaining')\n",
    "        plt.legend(loc = 'lower left')\n",
    "        plt.savefig(f'{experiment_name}/training_accuracy_{name}.png')\n",
    "\n",
    "        # Plot figures for validation accuracy\n",
    "        plt.figure()\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xscale('log')\n",
    "        plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "        for masktype in masktypes:\n",
    "            length = len(percent_removed[masktype])//num_trials\n",
    "            mean = []\n",
    "            std = []\n",
    "            for i in range(length):\n",
    "                mean.append(np.mean(valid_accuracies[masktype][i::length]))\n",
    "                std.append(1.96*np.std(valid_accuracies[masktype][i::length])/np.sqrt(num_trials))\n",
    "            plt.errorbar(percent_removed[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "        plt.ylabel('Validation accuracy')\n",
    "        plt.xlabel('Proportion of nodes/feature maps remaining')\n",
    "        plt.legend(loc = 'lower left')\n",
    "        plt.savefig(f'{experiment_name}/validation_accuracy_{name}.png')\n",
    "\n",
    "        # Plot figures for test accuracy\n",
    "        plt.figure()\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xscale('log')\n",
    "        plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "        for masktype in masktypes:\n",
    "            length = len(percent_removed[masktype])//num_trials\n",
    "            mean = []\n",
    "            std = []\n",
    "            for i in range(length):\n",
    "                mean.append(np.mean(test_accuracies[masktype][i::length]))\n",
    "                std.append(1.96*np.std(test_accuracies[masktype][i::length])/np.sqrt(num_trials))\n",
    "            plt.errorbar(percent_removed[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "        plt.ylabel('Test accuracy')\n",
    "        plt.xlabel('Proportion of nodes/feature maps remaining')\n",
    "        plt.legend(loc = 'lower left')\n",
    "        plt.savefig(f'{experiment_name}/test_accuracy_{name}.png')\n",
    "\n",
    "        if oracle:\n",
    "            # Plot figures for oracle comparison\n",
    "            plt.figure()\n",
    "            plt.gca().invert_xaxis()\n",
    "            plt.xscale('log')\n",
    "            plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "            for masktype in masktypes:\n",
    "                length = len(percent_removed[masktype])//num_trials\n",
    "                mean = []\n",
    "                std = []\n",
    "                for i in range(length):\n",
    "                    mean.append(np.mean(oracle_comparison[masktype][i::length]))\n",
    "                    std.append(1.96*np.std(oracle_comparison[masktype][i::length])/np.sqrt(num_trials))\n",
    "                plt.errorbar(percent_removed[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "            plt.ylabel('Test accuracy')\n",
    "            plt.xlabel('Proportion of nodes/feature maps remaining')\n",
    "            plt.legend(loc = 'lower left')\n",
    "            plt.savefig(f'{experiment_name}/oracle_comparison_{name}.png')\n",
    "\n",
    "        # Plot figures for early stopping iteration\n",
    "        plt.figure()\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.xscale('log')\n",
    "        plt.xticks([0.1,0.2,0.3,0.4,0.5, 0.6, 0.7, 0.8, 0.9, 1.0], ['0.1','0.2','0.3','0.4','0.5', '0.6', '0.7', '0.8', '0.9', '1.0'], rotation = 45)\n",
    "        for masktype in masktypes:\n",
    "            length = len(percent_removed[masktype])//num_trials\n",
    "            mean = []\n",
    "            std = []\n",
    "            for i in range(length):\n",
    "                mean.append(np.mean(early_stopping[masktype][i::length]))\n",
    "                std.append(1.96*np.std(early_stopping[masktype][i::length])/np.sqrt(num_trials))\n",
    "            plt.errorbar(percent_removed[masktype][:length], mean, yerr = std, fmt = fmt[masktype], capsize = 2, alpha = 0.5, color=colors[masktype], label = masktype)\n",
    "            plt.ylabel('Early stopping iteration')\n",
    "            plt.xlabel('Proportion of nodes/feature maps remaining')\n",
    "            plt.legend(loc = 'lower left')\n",
    "            plt.savefig(f'{experiment_name}/early_stopping_{name}.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d09QoewUPYob"
   },
   "source": [
    "## Section 1: Model A - Multilayer perceptron initialization for MNIST \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2boajkX331Wc"
   },
   "outputs": [],
   "source": [
    "class MlpModelA(NodePrune):\n",
    "    \"\"\"MNIST\"\"\"\n",
    "    def __init__(self, number_nodes_layer_1=40, number_nodes_layer_2=40, dense=10):\n",
    "        self.number_nodes_layer_1 = number_nodes_layer_1\n",
    "        self.number_nodes_layer_2 = number_nodes_layer_2\n",
    "        self.dense = dense\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.get_dataset()\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        mnist = keras.datasets.mnist\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "        x_test = x_test/255.0\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # split into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    # We drop the nodes when creating a new model.\n",
    "    def initialize_model(self, mask=None):\n",
    "        \"\"\"\n",
    "        Initialize a multi-layer perceptron model. Droping or retention of nodes \n",
    "        happens when add_mask() is called.\n",
    "        Args:\n",
    "            mask: Boolean - if None, a mask dictionary is created, otherwise, mask\n",
    "            has already been created\n",
    "        Returns:\n",
    "            mask:  mask: Dict - key is a layer, value is numpy array of mask\n",
    "            model: a tf.keral functional Model made up of layers\n",
    "            activation_array: array of activations (i.e output of Relu(Wx + b))\n",
    "        \"\"\"\n",
    "        layer_index = 0\n",
    "        layer_name = 'layer_'\n",
    "        activation_array = []\n",
    "\n",
    "        if mask is None:\n",
    "            mask = {} \n",
    "            initialize = True\n",
    "        else:\n",
    "            initialize = False\n",
    "\n",
    "        inputs = Input(shape = [28, 28, 1], name=layer_name+str(layer_index)) \n",
    "        layer_index += 1\n",
    "        \n",
    "        model = Flatten(name=layer_name+str(layer_index))(inputs)\n",
    "        mask, model, activation_array, layer_index = self.add_mask(mask, model, layer_index, activation_array, initialize=initialize, in_out_layers=True)\n",
    "        model = Dense(40, activation='relu', name=layer_name+str(layer_index))(model) \n",
    "        mask, model, activation_array, layer_index = self.add_mask(mask, model, layer_index, activation_array, initialize=initialize, in_out_layers=False)\n",
    "        model = Dense(40, activation='relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_mask(mask, model, layer_index, activation_array, initialize=initialize, in_out_layers=False)\n",
    "        out = Dense(10, activation='softmax', name=layer_name+str(layer_index))(model)\n",
    "        out2 = Dense(10, activation='relu', name=layer_name+str(layer_index))(model) #the output layer is needed\n",
    "        mask, model, activation_array, layer_index = self.add_mask(mask, out, layer_index, activation_array, initialize=initialize, in_out_layers=True) \n",
    "        #overide the last activation which is softmax with relu\n",
    "        activation_array[-1] = Multiply()([out2, tf.multiply(tf.ones_like(out2), tf.ones_like(out2))])\n",
    "\n",
    "        model = Model(inputs = [inputs], outputs = [out])\n",
    "        activation_model = Model(inputs = [inputs], outputs = activation_array)\n",
    "        return mask, model, activation_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT8bDiZYaLW1"
   },
   "source": [
    "## Section 2: Model B - CNN initialization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8KgKcndaS0H"
   },
   "outputs": [],
   "source": [
    "class CnnModelB(NodePrune):\n",
    "    \"\"\"MNIST\"\"\"\n",
    "    def __init__(self, number_filters_conv_1=64, number_filters_conv_2=64, dense=10):\n",
    "        self.number_filters_conv_1 = number_filters_conv_1\n",
    "        self.number_filters_conv_2 = number_filters_conv_2\n",
    "        self.dense = dense\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.get_dataset()\n",
    "\n",
    "    def get_dataset(self):\n",
    "        mnist = keras.datasets.mnist\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "        x_test = x_test/255.0\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # split into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    def initialize_model(self, mask = None):\n",
    "        \"\"\"\n",
    "        Initialize a cnn model. Droping or retention of filters \n",
    "        happens when add_filter_mask() is called.\n",
    "        Args:\n",
    "            mask: Boolean - if None, a mask dictionary is created, otherwise, mask\n",
    "            has already been created\n",
    "        Returns:\n",
    "            mask:  mask: Dict - key is a layer, value is numpy array of mask\n",
    "            model: a tf.keral functional Model made up of layers\n",
    "            activation_array: array of activations (i.e output of Relu(Wx + b))\n",
    "        \"\"\"\n",
    "        layer_index = 0\n",
    "        layer_name = 'layer_'\n",
    "        activation_array = []\n",
    "        \n",
    "        # if no mask specified, start with no mask\n",
    "        if mask is None:\n",
    "            mask = {}\n",
    "            initialize = True\n",
    "        else:\n",
    "            initialize = False\n",
    "        \n",
    "        inputs = Input(shape = [28, 28, 1], name=layer_name+str(layer_index)) \n",
    "        layer_index += 1\n",
    "        model = Conv2D(self.number_filters_conv_1, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(inputs)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize=initialize) \n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = Conv2D(self.number_filters_conv_2, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize=initialize)   \n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1    \n",
    "        model = Flatten(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        out = Dense(self.dense, activation = 'softmax', name=layer_name+str(layer_index))(model)\n",
    "        activation_array.append(out)\n",
    "        \n",
    "        model = Model(inputs = [inputs], outputs = [out])\n",
    "        activation_model = Model(inputs = [inputs], outputs = activation_array)\n",
    "        return mask, model, activation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJVuFhOVanIX"
   },
   "source": [
    "## Section 3: Model C - CNN initialization for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mquQiCJnatBm"
   },
   "outputs": [],
   "source": [
    "class CnnModelC(NodePrune):\n",
    "    \"\"\"CIFAR-10\"\"\"\n",
    "    def __init__(self, number_filters_conv_1=64, number_filters_conv_2=64, number_filters_conv_3=128, number_filters_conv_4=128, dense_1=256, dense_2=10):\n",
    "        self.number_filters_conv_1 = number_filters_conv_1\n",
    "        self.number_filters_conv_2 = number_filters_conv_2\n",
    "        self.number_filters_conv_3 = number_filters_conv_3\n",
    "        self.number_filters_conv_4 = number_filters_conv_4\n",
    "        self.dense_1 = dense_1\n",
    "        self.dense_2 = dense_2\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.get_dataset()\n",
    "\n",
    "    def get_dataset(self):\n",
    "        cifar = keras.datasets.cifar10\n",
    "        (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "        x_train = x_train.reshape(-1, 32, 32, 3)\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test.reshape(-1, 32, 32, 3)\n",
    "        x_test = x_test/255.0\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # split into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    \n",
    "    def initialize_model(self, mask = None):\n",
    "        \"\"\"\n",
    "        Initialize a cnn model with a certain mask. Droping or retention of filters \n",
    "        happens when add_filter_mask() is called.\n",
    "        Args:\n",
    "            mask: Boolean - if None, a mask dictionary is created, otherwise, mask\n",
    "            has already been created\n",
    "        Returns:\n",
    "            mask:  mask: Dict - key is a layer, value is numpy array of mask\n",
    "            model: a tf.keral functional Model made up of layers\n",
    "            activation_array: array of activations (i.e output of Relu(Wx + b))\n",
    "        \"\"\"\n",
    "        layer_index = 0\n",
    "        layer_name = 'layer_'\n",
    "        activation_array = []\n",
    "        \n",
    "        # if no mask specified, start with no mask\n",
    "        if mask is None:\n",
    "            mask = {}\n",
    "            initialize = True\n",
    "        else:\n",
    "            initialize = False\n",
    "        \n",
    "        inputs = Input(shape = [32, 32, 3], name=layer_name+str(layer_index))\n",
    "        layer_index += 1\n",
    "\n",
    "        ## Define your model architecture below\n",
    "        ## For every Conv Layer, follow up with an add_filter_mask line to add the filter mask\n",
    "        \n",
    "        # Model C (Conv layers)\n",
    "        model = Conv2D(self.number_filters_conv_1, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(inputs)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = Conv2D(self.number_filters_conv_2, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(self.number_filters_conv_3, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = Conv2D(self.number_filters_conv_4, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = Flatten(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        # Multiply by mask on nodes\n",
    "        model = Dense(self.dense_1, activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        out = Dense(self.dense_2, activation = 'softmax', name=layer_name+str(layer_index))(model)\n",
    "        activation_array.append(out)\n",
    "        \n",
    "        model = Model(inputs = [inputs], outputs = [out])\n",
    "        activation_model = Model(inputs = [inputs], outputs = activation_array)\n",
    "        \n",
    "        return mask, model, activation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSPOpzTna7Ua"
   },
   "source": [
    "## Section 4: ResNet18 initialization for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYr-stiZa8vO"
   },
   "outputs": [],
   "source": [
    "class ResNet18(NodePrune):\n",
    "    \"\"\"CIFAR-10\"\"\"\n",
    "    def __init__(self):\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.get_dataset()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        cifar = keras.datasets.cifar10\n",
    "        (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "        x_train = x_train.reshape(-1, 32, 32, 3)\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test.reshape(-1, 32, 32, 3)\n",
    "        x_test = x_test/255.0\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # split into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    def initialize_model(self, mask = None):\n",
    "        \"\"\"\n",
    "        Initialize a ResNet-18 model with a certain model. Droping or retention of filters \n",
    "        happens when add_filter_mask() is called.\n",
    "        Args:\n",
    "            mask: Boolean - if None, a mask dictionary is created, otherwise, mask\n",
    "            has already been created\n",
    "        Returns:\n",
    "            mask:  mask: Dict - key is a layer, value is numpy array of mask\n",
    "            model: a tf.keral functional Model made up of layers\n",
    "            activation_array: array of activations (i.e output of Relu(Wx + b))\n",
    "        \"\"\"\n",
    "        layer_index = 0\n",
    "        layer_name = 'layer_'\n",
    "        activation_array = []\n",
    "\n",
    "        # if no mask specified, start with no mask\n",
    "        if mask is None:\n",
    "            mask = {}\n",
    "            initialize = True\n",
    "        else:\n",
    "            initialize = False\n",
    "        \n",
    "        inputs = Input(shape = [32, 32, 3], name=layer_name+str(layer_index))\n",
    "        layer_index += 1\n",
    "\n",
    "        # # [64] x 1, 1/2\n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(inputs)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)  \n",
    "        shortcut = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        # [64, 64] x 2, 1/2\n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "        shortcut = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model) \n",
    "        layer_index += 1\n",
    "\n",
    "        # [128, 128] x 2, 1/2\n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut) \n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model) \n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut) \n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, shortcut, activation_array, layer_index = self.add_filter_mask(mask, shortcut, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Add(name=layer_name+str(layer_index))([shortcut, model]) \n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut) \n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "        shortcut = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        # [256, 256] x 2, 1/2\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, shortcut, activation_array, layer_index = self.add_filter_mask(mask, shortcut, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "        shortcut = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        # [512, 512] x 2, 1/2\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, shortcut, activation_array, layer_index = self.add_filter_mask(mask, shortcut, layer_index, activation_array, initialize = initialize)\n",
    "        shortcut = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(shortcut)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        #model = BatchNormalization()(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Add(name=layer_name+str(layer_index))([shortcut, model])\n",
    "        layer_index += 1\n",
    "\n",
    "        model = GlobalAveragePooling2D(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1  \n",
    "        # Multiply by mask on nodes (Drop nodes)\n",
    "        out = Dense(10, activation = 'softmax', name=layer_name+str(layer_index))(model)\n",
    "        activation_array.append(out)\n",
    "        \n",
    "        model = Model(inputs = [inputs], outputs = [out])\n",
    "        activation_model = Model(inputs = [inputs], outputs = activation_array)\n",
    "\n",
    "        return mask, model, activation_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti4HgDBDbFCI"
   },
   "source": [
    "## Section 5: VGG19 initialization for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKWZ2Zs5bGKK"
   },
   "outputs": [],
   "source": [
    "class Vgg19(NodePrune):\n",
    "    \"\"\"CIFAR-10\"\"\"\n",
    "    def __init__(self):\n",
    "        self.x_train = None \n",
    "        self.x_val = None \n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        self.get_dataset()\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        cifar = keras.datasets.cifar10\n",
    "        (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "        x_train = x_train.reshape(-1, 32, 32, 3)\n",
    "        x_train = x_train/255.0\n",
    "        x_test = x_test.reshape(-1, 32, 32, 3)\n",
    "        x_test = x_test/255.0\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        # split into train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(x_train, y_train, stratify = y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "    def initialize_model(self, mask = None):\n",
    "        \"\"\"\n",
    "        Initialize a VGG-19 model with a certain mask. Droping or retention of filters \n",
    "        happens when add_filter_mask() is called.\n",
    "        Args:\n",
    "            mask: Boolean - if None, a mask dictionary is created, otherwise, mask\n",
    "            has already been created\n",
    "        Returns:\n",
    "            mask:  mask: Dict - key is a layer, value is numpy array of mask\n",
    "            model: a tf.keral functional Model made up of layers\n",
    "            activation_array: array of activations (i.e output of Relu(Wx + b))\n",
    "        \"\"\"\n",
    "        layer_index = 0\n",
    "        layer_name = 'layer_'\n",
    "        activation_array = []\n",
    "        \n",
    "        # if no mask specified, start with no mask\n",
    "        layer = 0\n",
    "        if mask is None:\n",
    "            mask = {}\n",
    "            initialize = True\n",
    "        else:\n",
    "            initialize = False\n",
    "        \n",
    "        inputs = Input(shape = [32, 32, 3], name=layer_name+str(layer_index))\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(inputs)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        model = Conv2D(64, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)   \n",
    "        layer_index += 1\n",
    "        model = BatchNormalization(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(128, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        layer_index += 1\n",
    "        model = BatchNormalization(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(256, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        layer_index += 1\n",
    "        model = BatchNormalization(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        layer_index += 1\n",
    "        model = BatchNormalization(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        \n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        model = Conv2D(512, (3, 3), padding = 'same', activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        mask, model, activation_array, layer_index = self.add_filter_mask(mask, model, layer_index, activation_array, initialize = initialize)\n",
    "        layer_index += 1\n",
    "        model = BatchNormalization(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        model = MaxPooling2D((2, 2), name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "        \n",
    "        model = Flatten(name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        # Multiply by mask on nodes (Drop nodes)\n",
    "        model = Dense(256, activation = 'relu', name=layer_name+str(layer_index))(model)\n",
    "        layer_index += 1\n",
    "\n",
    "        out = Dense(10, activation = 'softmax', name=layer_name+str(layer_index))(model)\n",
    "        activation_array.append(out)\n",
    "        \n",
    "        model = Model(inputs = [inputs], outputs = [out])\n",
    "        activation_model = Model(inputs = [inputs], outputs = activation_array)\n",
    "        \n",
    "        return mask, model, activation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXz9IgcubM8X"
   },
   "source": [
    "## Section 6: MNIST Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTofaLuhbOcr"
   },
   "outputs": [],
   "source": [
    "class MlpMnistExperiment():\n",
    "    \"\"\"MLP MNIST Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using MLP of different node configurations\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on MLP_40_40...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'mlp_mnist_modelA_dense40_dense40'\n",
    "\n",
    "        ## Model evaluation to compare all metrics \n",
    "        cache_of_mlp_model_40_40 = MlpModelA(40, 40).generate_model(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'mnist_dense40_dense40_evaluate'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_mlp_model_40_40, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_mlp_model_40_40, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")\n",
    "        \n",
    "        ## Change your model name accordingly\n",
    "        experiment_name = 'mlp_mnist_modelA_dense20_dense40'\n",
    "        print(f\"Evaluating on MLP_20_40...\")\n",
    "        cache_of_mlp_model_20_40 = MlpModelA(20, 40).generate_model(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        model_name = 'mnist_dense20_dense40_evaluate'\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_mlp_model_20_40, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_mlp_model_20_40, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")\n",
    "        \n",
    "        ## Change your model name accordingly\n",
    "        experiment_name = 'mlp_mnist_modelA_dense40_dense20'\n",
    "        print(f\"Evaluating on MLP_40_20...\")\n",
    "        cache_of_mlp_model_40_20 = MlpModelA(40, 20).generate_model(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        model_name = 'mnist_dense40_dense20_evaluate'\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_mlp_model_40_20, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_mlp_model_40_20, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCKqPKhAbYDC"
   },
   "outputs": [],
   "source": [
    "class CnnMnistExperimentModelB_64_64():\n",
    "    \"\"\"CNN MNIST Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using Model B\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on Model B Conv64-Conv64...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'cnn_mnist_modelB_conv64_conv64'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_cnn_modelB_conv64_conv64 = CnnModelB(64, 64).generate_model(experiment_name, num_epochs= 100, num_trials=2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'cnn_mninst_conv64_conv64_evaluate'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_modelB_conv64_conv64, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_modelB_conv64_conv64, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E3hziolba4_"
   },
   "outputs": [],
   "source": [
    "class CnnMnistExperimentModelB_32_64():\n",
    "    \"\"\"CNN MNIST Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using Model B\n",
    "        Arguments:\n",
    "            self: current object\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Change your model name accordingly\n",
    "        experiment_name = 'cnn_mnist_modelB_conv32_conv64'\n",
    "        print(f\"Evaluating on Model B Conv32-Conv64...\")\n",
    "        cache_of_cnn_modelB_conv32_conv64 = CnnModelB(32, 64).generate_model(experiment_name, num_epochs= 100, num_trials=2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        model_name = 'cnn_mninst_conv32_conv64_evaluate'\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_modelB_conv32_conv64, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_modelB_conv32_conv64, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P46thWWQbdn6"
   },
   "outputs": [],
   "source": [
    "class CnnMnistExperimentModelB_64_32():\n",
    "    \"\"\"CNN MNIST Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using Model B\n",
    "        Arguments:\n",
    "            self: current object\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Change your model name accordingly\n",
    "        experiment_name = 'cnn_mnist_modelB_conv64_conv32'\n",
    "        print(f\"Evaluating on Model B Conv64-Conv32...\")\n",
    "        cache_of_cnn_modelB_conv64_conv32 = CnnModelB(64, 32).generate_model(experiment_name, num_epochs= 100, num_trials=2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        model_name = 'cnn_mninst_conv64_conv32_evaluate'\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_modelB_conv64_conv32, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_modelB_conv64_conv32, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JhekRV1bmHE"
   },
   "source": [
    "## Section 7: CIFAR-10 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71kQ8NLKbpqJ"
   },
   "outputs": [],
   "source": [
    "class CnnCifar10ExperimentModelC_64_64_128_128():\n",
    "    \"\"\"CNN CIFAR-10 Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using Model C\n",
    "        Arguments:\n",
    "            self: current object. \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on Model C Conv64-Conv64-Conv128-Conv128...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'cnn_cifar10_modelC_conv64_conv64_conv128_conv128'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_cnn_modelC_conv64_conv64_conv128_conv128 = CnnModelC(64, 64, 128, 128).generate_model(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'cnn_cifar10_conv64_conv64_conv128_conv128_evaluate'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_modelC_conv64_conv64_conv128_conv128, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_modelC_conv64_conv64_conv128_conv128, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRkVqGCMbvHQ"
   },
   "outputs": [],
   "source": [
    "class CnnCifar10ExperimentModelC_128_128_256_256():\n",
    "    \"\"\"CNN CIFAR-10 Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using Model C\n",
    "        Arguments:\n",
    "            self: current object. \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on Model C Conv128-Conv128-Conv256-Conv256...\")        \n",
    "        ## Change your model name accordingly\n",
    "        experiment_name = 'cnn_cifar10_modelC_conv128_conv128_conv256_conv256'\n",
    "        cache_of_cnn_modelC_conv128_conv128_conv256_conv256 = CnnModelC(128, 128, 256, 256).generate_model(experiment_name, num_epochs=100, num_trials=2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        model_name = 'cnn_cifar10_conv128_conv128_conv256_conv256_evaluate'\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_modelC_conv128_conv128_conv256_conv256, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_modelC_conv128_conv128_conv256_conv256, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpVASw0abx_h"
   },
   "outputs": [],
   "source": [
    "class CnnCifar10ResNet18Experiment():\n",
    "    \"\"\"CNN CIFAR-10 ResNet18 Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using MLP of different node configurations\n",
    "        Arguments:\n",
    "            print_progress: Boolean. If true, execution progress will be printed, if false, nothing will be printed.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on ResNet18...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'cnn_cifar10_resnet18'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_cnn_cifar10_resnet18 = ResNet18().generate_model(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'cnn_cifar10_resnet18_evaluate'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_cifar10_resnet18, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_cifar10_resnet18, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJbZk3FJb0Ar"
   },
   "outputs": [],
   "source": [
    "class CnnCifar10Vgg19Experiment():\n",
    "    \"\"\"CNN CIFAR-10 VGG19 Experiments\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset using MLP of different node configurations\n",
    "        Arguments:\n",
    "            print_progress: Boolean. If true, execution progress will be printed, if false, nothing will be printed.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Evaluating on VGG19...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'cnn_cifar10_vgg19'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_cnn_cifar10_vgg19 = Vgg19().generate_model(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'cnn_cifar10_vgg19_evaluate'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_cnn_cifar10_vgg19, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_cnn_cifar10_vgg19, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaNm83xhcVzg"
   },
   "source": [
    "## Section 8: Oracle Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns0-I70Wcbqp"
   },
   "outputs": [],
   "source": [
    "class OracleComparisonModelA_20_20():\n",
    "    \"\"\"Oracle MNIST Experiments on ModelA\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset on Model A\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on MLP_20_20...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'oracle_mlp_mnist_modelA_dense20_dense20'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_oracle_mlp_model_20_20 = MlpModelA(20, 20).oracle_model(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'oracle_mlp_mnist_modelA_dense20_dense20'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_oracle_mlp_model_20_20, model_name, experiment_name, num_trials = 2, oracle = True)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_oracle_mlp_model_20_20, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCPvc3licubZ"
   },
   "outputs": [],
   "source": [
    "class OracleComparisonModelB_20_20():\n",
    "    \"\"\"Oracle MNIST Experiments on ModelB\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset on Model B\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"    \n",
    "        print(f\"Evaluating on Model B Conv20-Conv20...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'oracle_cnn_mnist_modelB_conv20_conv20'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_oracle_cnn_mnist_modelB_conv20_conv20 = CnnModelB(20, 20).oracle_model(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'oracle_cnn_mninst_modelB_conv20_conv20'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_oracle_cnn_mnist_modelB_conv20_conv20, model_name, experiment_name, num_trials = 2, oracle = True)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_oracle_cnn_mnist_modelB_conv20_conv20, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-ZfXYAAc2Ps"
   },
   "source": [
    "## Section 9: Random Initialization Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtLV-_2Dc4s1"
   },
   "outputs": [],
   "source": [
    "class RandomInitializationModelA_20_20():\n",
    "    \"\"\"Random Initialization on ModelA on MNIST\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST on Models A on MNIST\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on ModelA - MLP_20_20...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'randominit_mlp_mnist_modelA_dense20_dense20'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_randominit_mlp_mnist_model_20_20 = MlpModelA(20, 20).compare_random(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'randominit_mlp_mnist_modelA_dense20_dense20'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_randominit_mlp_mnist_model_20_20, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_randominit_mlp_mnist_model_20_20, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-VXlOocc73f"
   },
   "outputs": [],
   "source": [
    "class RandomInitializationModelB_64_64():\n",
    "    \"\"\"Random Initialization on ModelB on MNIST\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST on Model B\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on Model B Conv64-Conv64...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'randominit_cnn_mnist_modelB_conv64_conv64'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_randominit_cnn_modelB_conv64_conv64 = CnnModelB(64, 64).compare_random(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'randominit_cnn_modelB_mninst_conv64_conv64'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_randominit_cnn_modelB_conv64_conv64, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_randominit_cnn_modelB_conv64_conv64, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SovBgn1yc-KH"
   },
   "outputs": [],
   "source": [
    "class RandomInitializationModelC_64_64_128_128():\n",
    "    \"\"\"Random Initialization on ModelC on CIFAR-10\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST on CIFAR-10 on Model C\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on Model C Conv64-Conv64-Conv128-Conv128...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'randominit_cnn_cifar10_modelC_conv64_conv64_conv128_conv128'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_randominit_cnn_modelC_conv64_conv64_conv128_conv128 = CnnModelC(64, 64, 128, 128).compare_random(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'randominit_cnn_cifar10_conv64_conv64_conv128_conv128'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_randominit_cnn_modelC_conv64_conv64_conv128_conv128, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_randominit_cnn_modelC_conv64_conv64_conv128_conv128, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HnswQNYdGON"
   },
   "source": [
    "## Section 10: Percentage of nodes/filters to Drop Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdf7BzIedHjI"
   },
   "outputs": [],
   "source": [
    "class PercentageToDropModelA_20_20():\n",
    "    \"\"\"Random Initialization on Model A on MNIST\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset on MLP Model A \n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on ModelA - MLP_20_20...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'percentage_mlp_mnist_modelA_dense20_dense20'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_percentage_mlp_mnist_model_20_20 = MlpModelA(20, 20).compare_percent_mask(experiment_name, num_epochs=100, num_trials = 2, cnn=False)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'percentage_mlp_mnist_modelA_dense20_dense20'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_percentage_mlp_mnist_model_20_20, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_percentage_mlp_mnist_model_20_20, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksdr5-nEdKH0"
   },
   "outputs": [],
   "source": [
    "class PercentageToDropModelB_64_64():\n",
    "    \"\"\"Random Initialization on Model B on MNIST\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset on Model B\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"        \n",
    "        print(f\"Evaluating on Model B Conv64-Conv64...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'percentage_cnn_mnist_modelB_conv64_conv64'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_percentage_cnn_modelB_conv64_conv64 = CnnModelB(64, 64).compare_percent_mask(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'percentage_cnn_modelB_mninst_conv64_conv64'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_percentage_cnn_modelB_conv64_conv64, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_percentage_cnn_modelB_conv64_conv64, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wRg6XAkdM8V"
   },
   "outputs": [],
   "source": [
    "class PercentageToDropModelC_64_64_128_128():\n",
    "    \"\"\"Random Initialization on ModelC on CIFAR-10\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the experiments on the MNIST dataset on Model C\n",
    "        Arguments:\n",
    "            self: current object.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating on Model C Conv64-Conv64-Conv128-Conv128...\")\n",
    "        ## Change your folder name accordingly\n",
    "        experiment_name = 'percentage_cnn_cifar10_modelC_conv64_conv64_conv128_conv128'\n",
    "\n",
    "        ## Model evaluation to compare all metrics\n",
    "        cache_of_percentage_cnn_modelC_conv64_conv64_conv128_conv128 = CnnModelC(64, 64, 128, 128).compare_percent_mask(experiment_name, num_epochs=100, num_trials = 2)\n",
    "        print(f\"Evaluation on done.\")\n",
    "\n",
    "        ## Change your model name accordingly\n",
    "        model_name = 'percentage_cnn_cifar10_conv64_conv64_conv128_conv128'\n",
    "    \n",
    "        ## If want to print oraclecomparison graph, set to oracle = True (only for oraclemodel)\n",
    "        print(f\"Printing graph ...\")\n",
    "        NodePrune().print_graph(cache_of_percentage_cnn_modelC_conv64_conv64_conv128_conv128, model_name, experiment_name, num_trials = 2, oracle = False)\n",
    "        print(f\"Graph printed. Saving cache ...\")\n",
    "        NodePrune().save_file(cache_of_percentage_cnn_modelC_conv64_conv64_conv128_conv128, model_name)\n",
    "        print(f\"Cache saved as {model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFbhGwQQdSCz"
   },
   "source": [
    "## Section: Main - entry point of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpPSCs6zdWUd"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Experiments\"\"\"\n",
    "    MlpMnistExperiment().run()\n",
    "    #CnnMnistExperimentModelB_64_64().run()\n",
    "    #CnnMnistExperimentModelB_32_64().run()\n",
    "    #CnnMnistExperimentModelB_64_32().run()\n",
    "    #CnnCifar10ExperimentModelC_64_64_128_128().run()\n",
    "    #CnnCifar10ExperimentModelC_128_128_256_256().run()\n",
    "    #CnnCifar10ResNet18Experiment().run()\n",
    "    #CnnCifar10Vgg19Experiment().run()\n",
    "    #OracleComparisonModelA_20_20().run()\n",
    "    #OracleComparisonModelB_20_20().run()\n",
    "    #RandomInitializationModelA_20_20().run()\n",
    "    #RandomInitializationModelB_64_64().run()\n",
    "    #RandomInitializationModelC_64_64_128_128().run()\n",
    "    #PercentageToDropModelA_20_20().run()\n",
    "    #PercentageToDropModelB_64_64().run()\n",
    "    #PercentageToDropModelC_64_64_128_128().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d09QoewUPYob",
    "fT8bDiZYaLW1",
    "QJVuFhOVanIX",
    "jSPOpzTna7Ua",
    "Ti4HgDBDbFCI",
    "QXz9IgcubM8X",
    "8JhekRV1bmHE",
    "JaNm83xhcVzg",
    "R-ZfXYAAc2Ps",
    "-HnswQNYdGON"
   ],
   "name": "node_prune2_oop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-gpu (tf_gpu_env)",
   "language": "python",
   "name": "tf_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
